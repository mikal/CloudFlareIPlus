#!/usr/bin/env python3

def check_m3u_file(file_path):
    """Comprehensive check of M3U file"""
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    issues = []
    stats = {
        'total_lines': len(lines),
        'channels': 0,
        'extinf_lines': 0,
        'url_lines': 0,
        'http_urls': 0,
        'empty_lines': 0,
        'comment_lines': 0
    }
    
    # Check header
    if not lines[0].strip().startswith('#EXTM3U'):
        issues.append("æ–‡ä»¶ä¸ä»¥ #EXTM3U å¼€å¤´")
    
    # Analyze each line
    orphaned_extinf = []
    orphaned_urls = []
    duplicate_urls = {}
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        if not line:
            stats['empty_lines'] += 1
        elif line.startswith('#EXTINF:'):
            stats['extinf_lines'] += 1
            # Check if next line is URL
            if i + 1 < len(lines):
                next_line = lines[i + 1].strip()
                if next_line.startswith('http'):
                    stats['channels'] += 1
                    stats['http_urls'] += 1
                    # Check for duplicates
                    if next_line in duplicate_urls:
                        duplicate_urls[next_line] += 1
                    else:
                        duplicate_urls[next_line] = 1
                    i += 2
                    continue
                else:
                    orphaned_extinf.append(i + 1)
            else:
                orphaned_extinf.append(i + 1)
        elif line.startswith('http'):
            stats['url_lines'] += 1
            orphaned_urls.append(i + 1)
        elif line.startswith('#'):
            stats['comment_lines'] += 1
        
        i += 1
    
    # Find actual duplicates
    duplicates = {url: count for url, count in duplicate_urls.items() if count > 1}
    
    # Report issues
    if orphaned_extinf:
        issues.append(f"å‘ç° {len(orphaned_extinf)} ä¸ªå­¤ç«‹çš„ #EXTINF è¡Œ (ç¼ºå°‘URL): {orphaned_extinf[:5]}")
    
    if orphaned_urls:
        issues.append(f"å‘ç° {len(orphaned_urls)} ä¸ªå­¤ç«‹çš„ URL è¡Œ (ç¼ºå°‘#EXTINF): {orphaned_urls[:5]}")
    
    if duplicates:
        issues.append(f"å‘ç° {len(duplicates)} ä¸ªé‡å¤çš„URL")
    
    return stats, issues, duplicates

def main():
    file_path = '/home/mike/iptv/SHIPTV2026-11.m3u'
    
    print(f"æ£€æŸ¥æ–‡ä»¶: {file_path}")
    
    stats, issues, duplicates = check_m3u_file(file_path)
    
    print(f"\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡:")
    print(f"  æ€»è¡Œæ•°: {stats['total_lines']}")
    print(f"  é¢‘é“æ•°: {stats['channels']}")
    print(f"  #EXTINFè¡Œ: {stats['extinf_lines']}")
    print(f"  HTTP URLè¡Œ: {stats['http_urls']}")
    print(f"  æ³¨é‡Šè¡Œ: {stats['comment_lines']}")
    print(f"  ç©ºè¡Œ: {stats['empty_lines']}")
    
    if issues:
        print(f"\nâš ï¸  å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. {issue}")
    else:
        print(f"\nâœ… æ–‡ä»¶æ ¼å¼æ­£å¸¸ï¼Œæ— é—®é¢˜å‘ç°")
    
    if duplicates:
        print(f"\nğŸ”„ é‡å¤URLè¯¦æƒ…:")
        for url, count in list(duplicates.items())[:5]:
            print(f"  {url} (å‡ºç°{count}æ¬¡)")
        if len(duplicates) > 5:
            print(f"  ... è¿˜æœ‰ {len(duplicates) - 5} ä¸ªé‡å¤URL")

if __name__ == "__main__":
    main()
